{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias - Variance dillema - why we use validation?\n",
    "How can we be sure that our algorithm is really representing the round truth of $F(X)$ and is not meorizing the dataset $y$? We now that it is impossible to *truly* know the $F(X)$. In most cases we will also not memorize the dataset. Created estimator $\\hat{F(X))}$ will be something inbetween.\n",
    "\n",
    "<img src=\"img/biasVarianceCurve.jpg\" width=\"70%\">\n",
    "Source: https://www.quora.com/What-is-the-best-way-to-explain-the-bias-variance-trade-off-in-layman%E2%80%99s-terms\n",
    "\n",
    "This problem is called Bias-Variance tradeoff. The theory behind it is well established and described with mathematical precision in the literature. Here we will focus on the intuition.\n",
    "\n",
    "The more freedom we give to our estimator (more elasticity/variance) the easier/better it will fit the training data. Therefore it will achieve lower error (bias) on the training set. At the same time the higher the variability the higher the risk of overfitting. Although theoretically we can be lowering the expected value of error (bias) it is usually done with the increase of error variance. We can land in a situation where, when we make prediction on new data, the error is very high due to variance (even though it can be zero on average).\n",
    "\n",
    "To understand it lets look at this picture:\n",
    "<img src=\"img/biasVarianceTarget.jpg\" width=\"30%\">\n",
    "Source: https://www.quora.com/What-is-the-best-way-to-explain-the-bias-variance-trade-off-in-layman%E2%80%99s-terms\n",
    "\n",
    "In practice higher variance usually means higher error (on new data). This problem is perfectly visible in the following illustration.\n",
    "<img src=\"img/biasVarianceValid.png\" width=\"50%\">\n",
    "Source: Elements of Statistical learning\n",
    "\n",
    "The red line represents the error on \"new\" data and blue line on the training data. When we look at this image it seems that the answer to the question of optimal complexity. In the end we want our mdoel to work as good as possible on new data and not on training data. It seems that chosing the complexity level for which the red line is the lowest is optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation (CV)\n",
    "Cross validation is in sesence a process described above. We split the data into a training and validation samples. We use training data to create our estimator and validation data do check how it will work on \"new\", out of sample data.\n",
    "<img src=\"img/cv.jpg\">\n",
    "Source: Wikipedia\n",
    "\n",
    "In such process a lot depends on the realization of the random split. The results may vary heavily on different realizations. We cannot treat the value in a single validation as a very good approximation of ground truth.\n",
    "\n",
    "### Cross Validation - what for?\n",
    "We use cross validation mainly  for one of two reasins:\n",
    "* Getting an objective evaluation of \"real\" effectivness of our model.\n",
    "* Tuning of models hyperparameters. Hyperaparameters are those arguments that we need to send by hand, are not part of algorithm optimization. Model complexity is often on of those hyper parameters as in the example above.\n",
    "\n",
    "### K-Fold Cross validation (k-fold CV)\n",
    "To get more reliable evaluation of our model a k-fold cross validation is used. We repeat the process of CV K times in such a way that 1/K of all $y$ observations are used for validation and the rest for training. We repeat the procedure K times in such a way that every observation is used for validation only once.\n",
    "<img src=\"img/cvKfold.jpg\">\n",
    "Source: Wikipedia\n",
    "\n",
    "Most common values for K are 5 or 10. In the end a result of k-foldcv is the average value of our metric across all validation sets.\n",
    "\n",
    "*Notice.* The result of k-fold cv will be different each time as the process of splitting is random.\n",
    "\n",
    "### Leave-One-Out Cross Validation (LOO CV)\n",
    "The moste extreme form of k-fold cv is Leave-One-Out Cross Validation (LOO CV). In this case K is equal to N, where N is total number of observations. One might think that this must be \"the best\" cv method as it is moste detailed with a lot of averaging and our training data is as big as possible each time. However, there are shortcomings. Each time our estimator will be very similar as the differences between training data are small. This decreases the value of information we get in the cv process in this case as we do not test our algorithm in \"different scenarios\" (treining sets). One important charachteristic of LOO cv is that we get the same result each time we run it.\n",
    "\n",
    "### Repeated Cross Validation\n",
    "In most cases running a k-fold cv is good enough. Sometimes however we need to get as accurate evaluation as possible. We can take advantage of the randomness and simply run k-fold several times and average the results.\n",
    "\n",
    "### Bootstrap cross validation\n",
    "Lest cv technique is bootstrap. It works by sampling with replacement N observations from our data set of N observations as training data. As we draw with replacement some observations are repeated and some are not drawn at all. The samples that have not been drawn are treated as validation set. We repeat this procedure multiple times to get the distribution of evaluation metric on validation set.\n",
    "\n",
    "## CV Methids - which one to chose?\n",
    "There is no consensus about the best CV method. Neither among practicioners nor in academia. Theoretically it seems that k-fold (repeated) and bootstrap are two main choices. In everyday use k-fold seems most popular. It may be due to the fact that when we do bootstrap cv there is a good chance that some observations will never be part of validation set. Additionally some people see duplication of observations as an important shortcoming.\n",
    "\n",
    "If we decide to use k-fold there is one more question to answer. Should it be 5, 10 or some other k-fold? On oen hand we want our training set to be as big as possible to avoid overfitting. On the other we want to have some variabiliti of training set and stability of results in validation sets. Again there is no one best answer and one should test different options and do repeated k-fold if more precision is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to setup Cross Validation?\n",
    "Althuogh in theory the procedure of CV is very simple it is very easy to make mistakes that in some scenarios are very costly.\n",
    "\n",
    "### Stratify\n",
    "One of the problems we can encounter is the balance of labels in training and validation sets. This is especially important when we work on imbalanced data. In datasets where there are few \"1\" in general it is easy to imagine a situation in which we draw no or relatively very little \"1\"s. it can lead to bias and/or big variance of our evaluation metric on the validation data set. Because of this problem we often enforce equal balance using stratification.\n",
    "\n",
    "For example in case of 5-fold cv you can imagine that we make the split for traingin and validation sets separately for \"1\"s and \"0\"s. This quarantees equal balance in both groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiem series analysis\n",
    "The hardest task to set up proper cross validation is time series analysis. Due to the fact that all observations are usualy linked (not independant over time), each observation depends on previous realizations. Beacause of this property we cannot simply randomly draw observations into two samples and use different permutations. Observations that are next to each other in time contain simply too much information about each other to have them in different sets (train and validation).\n",
    "\n",
    "One solution is to make a natural time-based split. We use \"past\" pbservations as training data and \"future\" pbservations as validation set. Just like in the illustration below.\n",
    "<img src=\"img/cvTimeSeries.png\" width=\"50%\">\n",
    "Source: https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information leakage\n",
    "Independance of observation between training and validation set is a crucial requirement for the procedure to deliver expected results. Its importance is clearly visible in time series analysis. However it is also possible to have infomration leakage even in regular classification and regression problems when the independance requirement is not met.\n",
    "\n",
    "In this case, when observations are not independant we cannot treat the validation set as a true *out of sample* data. This results in underestimation of error on validation set and can lead to heavy overestimation and result in a model that simply doesn't work in deployment.\n",
    "\n",
    "Having mutiple observations for one person in our data set is one example of data structure in which it is easy to break independance of data. Other example might be usage of data from many persons from one houshold.\n",
    "\n",
    "The errors in setup of cross validation process are very easy to make and hard to spot. Even in competition on kaggle data leakage happens from time to time when the sets are prepared by experienced professionals. Always try to know your data and set up your cv with care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validate Test\n",
    "At this point we should point to one important problem with the statements made ad the begining. We said that we can use results from validation set to choose values for hyperparameters of our algorithm. However this means that we are transfering information from validation set to training (data leakage). Doing this, if abused, can lead to biased evaluation and sub optimal model. Usually the information leakage is small and insignificant. however in cases where we heavily rely on validation set for algorithm tuning we should use train, validate, test procedure. The idea is exactly the same as in validation but we add one more step. We save part of the data (test) at the begining that we do not use at all in training process. Then, in the end, when everything is ready we test our model with it.\n",
    "<img src=\"img/cvTVT.png\">\n",
    "Source:https://am207.github.io/2017/wiki/validation.html\n",
    "\n",
    "### Train Validate Test in practice\n",
    "Splitting our dataset in three parts, as everythin in machien learning, has both advantages and shortcomings. Obviously the main advantage is getting pure and reliable evaluation of the model. We cannot forget about disatvantages too:\n",
    "* Seperating the test set decreases the number of observations in our training set. It increases the risk of overfitting and cen results in a worse predictive power of our model.\n",
    "* In a basic approach we seperate only one part of the set as test. This means that we rely again on one realization of some  random process. This means that we can get very different results reach time even if we repeat whole training procedure exactly the same.\n",
    "\n",
    "Abovementioned problems have two possible solutions. We can do a k-fold train-test/validate. It reduces the variance of the results in test set. On the other hand it increases the complexity of our ML pipeline and dose not alleviate the problem of reduced number of observations for training.\n",
    "\n",
    "Alternatively we can say that our real test  set will come later. We will simply wait for new data from the market that will be generated at the time of our model creation. This type of test data is perfect as it comes from the future. This is often a good choice as correctly implemented repeated k-fold cv should bu fully sufficient to chose the best model and optimize its hyperparameters.\n",
    "\n",
    "In the end one needs to chose a proper solution for a specific problem remembering about the type of algorithms we are planning to use and understanding the business need for current ML problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation in Python\n",
    "Most of the methods for cross validation are really well implemented in sklearn.\n",
    "http://scikit-learn.org/stable/modules/cross_validation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-ticks')\n",
    "%matplotlib inline\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get back to our medical set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35072, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UMARSTAT</th>\n",
       "      <th>UCUREMP</th>\n",
       "      <th>UCURNINS</th>\n",
       "      <th>USATMED</th>\n",
       "      <th>URELATE</th>\n",
       "      <th>REGION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>HHID</th>\n",
       "      <th>FHOSP</th>\n",
       "      <th>FDENT</th>\n",
       "      <th>FEMER</th>\n",
       "      <th>FDOCT</th>\n",
       "      <th>UIMMSTAT</th>\n",
       "      <th>U_USBORN</th>\n",
       "      <th>UAGE</th>\n",
       "      <th>U_FTPT</th>\n",
       "      <th>U_WKSLY</th>\n",
       "      <th>U_HRSLY</th>\n",
       "      <th>U_USHRS</th>\n",
       "      <th>HEARNVAL</th>\n",
       "      <th>HOTHVAL</th>\n",
       "      <th>HRETVAL</th>\n",
       "      <th>HSSVAL</th>\n",
       "      <th>HWSVAL</th>\n",
       "      <th>UBRACE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>UEDUC3</th>\n",
       "      <th>CEYES</th>\n",
       "      <th>CHAIR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Never married</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>2</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>55616128</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>22</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>52.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>No HS diploma or GED</td>\n",
       "      <td>hazel</td>\n",
       "      <td>brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Separated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>2</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>54704000</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>30</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>52.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>31468</td>\n",
       "      <td>5950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31468</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>HS diploma or GED, no bachelor's degree</td>\n",
       "      <td>blue</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Married_live together</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>5</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>57874272</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>33</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>24700</td>\n",
       "      <td>11340</td>\n",
       "      <td>0</td>\n",
       "      <td>4920</td>\n",
       "      <td>24700</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>No HS diploma or GED</td>\n",
       "      <td>brown</td>\n",
       "      <td>brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Divorced</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Little dissatisfied</td>\n",
       "      <td>4</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>54106816</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>41</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>43.0</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>60000</td>\n",
       "      <td>39002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60000</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>No HS diploma or GED</td>\n",
       "      <td>brown</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Never married</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>0</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>54569152</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>34</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>52.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>55280</td>\n",
       "      <td>4200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55280</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>HS diploma or GED, no bachelor's degree</td>\n",
       "      <td>brown</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                UMARSTAT UCUREMP UCURNINS              USATMED  URELATE  \\\n",
       "0          Never married      No      Yes       Very satisfied        2   \n",
       "1              Separated     Yes       No       Very satisfied        2   \n",
       "2  Married_live together      No       No       Very satisfied        5   \n",
       "3               Divorced      No      Yes  Little dissatisfied        4   \n",
       "4          Never married     Yes       No       Very satisfied        0   \n",
       "\n",
       "    REGION STATE      HHID FHOSP  FDENT  FEMER  FDOCT         UIMMSTAT  \\\n",
       "0  Midwest    WI  55616128    No      0      0      0  US-born citizen   \n",
       "1  Midwest    WI  54704000    No      2      0      0  US-born citizen   \n",
       "2  Midwest    WI  57874272    No      0      1      0  US-born citizen   \n",
       "3  Midwest    WI  54106816    No      0      0      1  US-born citizen   \n",
       "4  Midwest    WI  54569152    No      2      0      0  US-born citizen   \n",
       "\n",
       "  U_USBORN  UAGE     U_FTPT  U_WKSLY  U_HRSLY  U_USHRS  HEARNVAL  HOTHVAL  \\\n",
       "0      Yes    22  Full-time     52.0       40       40         0        0   \n",
       "1      Yes    30  Full-time     52.0       40       40     31468     5950   \n",
       "2      Yes    33  Part-time     52.0       30       30     24700    11340   \n",
       "3      Yes    41  Part-time     43.0       40       25     60000    39002   \n",
       "4      Yes    34  Full-time     52.0       40       40     55280     4200   \n",
       "\n",
       "   HRETVAL  HSSVAL  HWSVAL UBRACE  GENDER  \\\n",
       "0        0       0       0  White  Female   \n",
       "1        0       0   31468  White  Female   \n",
       "2        0    4920   24700  White    Male   \n",
       "3        0       0   60000  Black  Female   \n",
       "4        0       0   55280  Black    Male   \n",
       "\n",
       "                                    UEDUC3  CEYES  CHAIR  \n",
       "0                     No HS diploma or GED  hazel  brown  \n",
       "1  HS diploma or GED, no bachelor's degree   blue  black  \n",
       "2                     No HS diploma or GED  brown  brown  \n",
       "3                     No HS diploma or GED  brown  black  \n",
       "4  HS diploma or GED, no bachelor's degree  brown  black  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical = pd.read_csv(\"data/medical_care.csv\")\n",
    "print(medical.shape)\n",
    "pd.set_option(\"display.max_columns\",50)\n",
    "medical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets encode UCURNINS variable and run the regression on our full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical[\"UCURNINS\"] = (medical.UCURNINS==\"Yes\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>UCURNINS</td>     <th>  No. Observations:  </th>  <td> 35072</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td> 35036</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>    35</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th>    <td>1.0</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -11126.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Tue, 20 Mar 2018</td> <th>  Deviance:          </th> <td>  22251.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>17:22:37</td>     <th>  Pearson chi2:      </th> <td>4.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>         <td>6</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                          <td></td>                             <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                         <td>    0.9019</td> <td>    0.216</td> <td>    4.166</td> <td> 0.000</td> <td>    0.478</td> <td>    1.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Married, do not live together]</th>         <td>   -0.4264</td> <td>    0.166</td> <td>   -2.563</td> <td> 0.010</td> <td>   -0.752</td> <td>   -0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Married_live together]</th>                 <td>   -0.8331</td> <td>    0.056</td> <td>  -14.774</td> <td> 0.000</td> <td>   -0.944</td> <td>   -0.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Never married]</th>                         <td>   -0.3337</td> <td>    0.064</td> <td>   -5.210</td> <td> 0.000</td> <td>   -0.459</td> <td>   -0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Partnership]</th>                           <td>    0.3966</td> <td>    0.085</td> <td>    4.677</td> <td> 0.000</td> <td>    0.230</td> <td>    0.563</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Separated]</th>                             <td>   -0.0527</td> <td>    0.095</td> <td>   -0.553</td> <td> 0.580</td> <td>   -0.239</td> <td>    0.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Unknown]</th>                               <td>    0.6720</td> <td>    0.388</td> <td>    1.732</td> <td> 0.083</td> <td>   -0.089</td> <td>    1.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Widowed]</th>                               <td>   -0.1727</td> <td>    0.144</td> <td>   -1.197</td> <td> 0.231</td> <td>   -0.455</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>USATMED[T.Little satisfied]</th>                       <td>   -0.4597</td> <td>    0.063</td> <td>   -7.324</td> <td> 0.000</td> <td>   -0.583</td> <td>   -0.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>USATMED[T.No opinion]</th>                             <td>    0.6913</td> <td>    0.098</td> <td>    7.025</td> <td> 0.000</td> <td>    0.498</td> <td>    0.884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>USATMED[T.Very dissatisfied]</th>                      <td>    0.5547</td> <td>    0.092</td> <td>    6.017</td> <td> 0.000</td> <td>    0.374</td> <td>    0.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>USATMED[T.Very satisfied]</th>                         <td>   -0.7287</td> <td>    0.062</td> <td>  -11.668</td> <td> 0.000</td> <td>   -0.851</td> <td>   -0.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.Northeast]</th>                               <td>   -0.0486</td> <td>    0.057</td> <td>   -0.858</td> <td> 0.391</td> <td>   -0.159</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.South]</th>                                   <td>    0.7221</td> <td>    0.047</td> <td>   15.439</td> <td> 0.000</td> <td>    0.630</td> <td>    0.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.West]</th>                                    <td>    0.4020</td> <td>    0.049</td> <td>    8.143</td> <td> 0.000</td> <td>    0.305</td> <td>    0.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FHOSP[T.Yes]</th>                                      <td>   -0.2438</td> <td>    0.075</td> <td>   -3.254</td> <td> 0.001</td> <td>   -0.391</td> <td>   -0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UIMMSTAT[T.Foreign-born, non-citizen]</th>             <td>    0.7271</td> <td>    0.089</td> <td>    8.172</td> <td> 0.000</td> <td>    0.553</td> <td>    0.901</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UIMMSTAT[T.US-born citizen]</th>                       <td>   -0.5706</td> <td>    0.080</td> <td>   -7.114</td> <td> 0.000</td> <td>   -0.728</td> <td>   -0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_FTPT[T.Part-time]</th>                               <td>    0.4966</td> <td>    0.058</td> <td>    8.526</td> <td> 0.000</td> <td>    0.382</td> <td>    0.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.Asian/Pacific Islander]</th>                  <td>   -0.9828</td> <td>    0.161</td> <td>   -6.104</td> <td> 0.000</td> <td>   -1.298</td> <td>   -0.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.Black]</th>                                   <td>   -0.4685</td> <td>    0.126</td> <td>   -3.710</td> <td> 0.000</td> <td>   -0.716</td> <td>   -0.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.White]</th>                                   <td>   -0.5119</td> <td>    0.118</td> <td>   -4.338</td> <td> 0.000</td> <td>   -0.743</td> <td>   -0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UEDUC3[T.HS diploma or GED, no bachelor's degree]</th> <td>    0.8375</td> <td>    0.053</td> <td>   15.828</td> <td> 0.000</td> <td>    0.734</td> <td>    0.941</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UEDUC3[T.No HS diploma or GED]</th>                    <td>    1.5713</td> <td>    0.063</td> <td>   24.881</td> <td> 0.000</td> <td>    1.448</td> <td>    1.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GENDER[T.Male]</th>                                    <td>    0.1413</td> <td>    0.039</td> <td>    3.652</td> <td> 0.000</td> <td>    0.065</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>URELATE</th>                                           <td>    0.0161</td> <td>    0.012</td> <td>    1.350</td> <td> 0.177</td> <td>   -0.007</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FDENT</th>                                             <td>   -0.3159</td> <td>    0.015</td> <td>  -21.188</td> <td> 0.000</td> <td>   -0.345</td> <td>   -0.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FEMER</th>                                             <td>    0.0871</td> <td>    0.022</td> <td>    3.976</td> <td> 0.000</td> <td>    0.044</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FDOCT</th>                                             <td>   -0.1353</td> <td>    0.008</td> <td>  -16.404</td> <td> 0.000</td> <td>   -0.151</td> <td>   -0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UAGE</th>                                              <td>   -0.0182</td> <td>    0.002</td> <td>   -9.766</td> <td> 0.000</td> <td>   -0.022</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_WKSLY</th>                                           <td>   -0.0197</td> <td>    0.002</td> <td>  -12.732</td> <td> 0.000</td> <td>   -0.023</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_USHRS</th>                                           <td>    0.0022</td> <td>    0.002</td> <td>    1.234</td> <td> 0.217</td> <td>   -0.001</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HOTHVAL</th>                                           <td>-1.179e-07</td> <td> 1.57e-06</td> <td>   -0.075</td> <td> 0.940</td> <td>-3.19e-06</td> <td> 2.95e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HRETVAL</th>                                           <td> 5.256e-06</td> <td> 3.18e-06</td> <td>    1.650</td> <td> 0.099</td> <td>-9.86e-07</td> <td> 1.15e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HSSVAL</th>                                            <td>-5.631e-06</td> <td> 3.77e-06</td> <td>   -1.494</td> <td> 0.135</td> <td> -1.3e-05</td> <td> 1.76e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HWSVAL</th>                                            <td>  9.47e-09</td> <td> 3.24e-07</td> <td>    0.029</td> <td> 0.977</td> <td>-6.26e-07</td> <td> 6.45e-07</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:               UCURNINS   No. Observations:                35072\n",
       "Model:                            GLM   Df Residuals:                    35036\n",
       "Model Family:                Binomial   Df Model:                           35\n",
       "Link Function:                  logit   Scale:                             1.0\n",
       "Method:                          IRLS   Log-Likelihood:                -11126.\n",
       "Date:                Tue, 20 Mar 2018   Deviance:                       22251.\n",
       "Time:                        17:22:37   Pearson chi2:                 4.05e+04\n",
       "No. Iterations:                     6                                         \n",
       "=====================================================================================================================\n",
       "                                                        coef    std err          z      P>|z|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                             0.9019      0.216      4.166      0.000       0.478       1.326\n",
       "UMARSTAT[T.Married, do not live together]            -0.4264      0.166     -2.563      0.010      -0.752      -0.100\n",
       "UMARSTAT[T.Married_live together]                    -0.8331      0.056    -14.774      0.000      -0.944      -0.723\n",
       "UMARSTAT[T.Never married]                            -0.3337      0.064     -5.210      0.000      -0.459      -0.208\n",
       "UMARSTAT[T.Partnership]                               0.3966      0.085      4.677      0.000       0.230       0.563\n",
       "UMARSTAT[T.Separated]                                -0.0527      0.095     -0.553      0.580      -0.239       0.134\n",
       "UMARSTAT[T.Unknown]                                   0.6720      0.388      1.732      0.083      -0.089       1.433\n",
       "UMARSTAT[T.Widowed]                                  -0.1727      0.144     -1.197      0.231      -0.455       0.110\n",
       "USATMED[T.Little satisfied]                          -0.4597      0.063     -7.324      0.000      -0.583      -0.337\n",
       "USATMED[T.No opinion]                                 0.6913      0.098      7.025      0.000       0.498       0.884\n",
       "USATMED[T.Very dissatisfied]                          0.5547      0.092      6.017      0.000       0.374       0.735\n",
       "USATMED[T.Very satisfied]                            -0.7287      0.062    -11.668      0.000      -0.851      -0.606\n",
       "REGION[T.Northeast]                                  -0.0486      0.057     -0.858      0.391      -0.159       0.062\n",
       "REGION[T.South]                                       0.7221      0.047     15.439      0.000       0.630       0.814\n",
       "REGION[T.West]                                        0.4020      0.049      8.143      0.000       0.305       0.499\n",
       "FHOSP[T.Yes]                                         -0.2438      0.075     -3.254      0.001      -0.391      -0.097\n",
       "UIMMSTAT[T.Foreign-born, non-citizen]                 0.7271      0.089      8.172      0.000       0.553       0.901\n",
       "UIMMSTAT[T.US-born citizen]                          -0.5706      0.080     -7.114      0.000      -0.728      -0.413\n",
       "U_FTPT[T.Part-time]                                   0.4966      0.058      8.526      0.000       0.382       0.611\n",
       "UBRACE[T.Asian/Pacific Islander]                     -0.9828      0.161     -6.104      0.000      -1.298      -0.667\n",
       "UBRACE[T.Black]                                      -0.4685      0.126     -3.710      0.000      -0.716      -0.221\n",
       "UBRACE[T.White]                                      -0.5119      0.118     -4.338      0.000      -0.743      -0.281\n",
       "UEDUC3[T.HS diploma or GED, no bachelor's degree]     0.8375      0.053     15.828      0.000       0.734       0.941\n",
       "UEDUC3[T.No HS diploma or GED]                        1.5713      0.063     24.881      0.000       1.448       1.695\n",
       "GENDER[T.Male]                                        0.1413      0.039      3.652      0.000       0.065       0.217\n",
       "URELATE                                               0.0161      0.012      1.350      0.177      -0.007       0.039\n",
       "FDENT                                                -0.3159      0.015    -21.188      0.000      -0.345      -0.287\n",
       "FEMER                                                 0.0871      0.022      3.976      0.000       0.044       0.130\n",
       "FDOCT                                                -0.1353      0.008    -16.404      0.000      -0.151      -0.119\n",
       "UAGE                                                 -0.0182      0.002     -9.766      0.000      -0.022      -0.015\n",
       "U_WKSLY                                              -0.0197      0.002    -12.732      0.000      -0.023      -0.017\n",
       "U_USHRS                                               0.0022      0.002      1.234      0.217      -0.001       0.006\n",
       "HOTHVAL                                           -1.179e-07   1.57e-06     -0.075      0.940   -3.19e-06    2.95e-06\n",
       "HRETVAL                                            5.256e-06   3.18e-06      1.650      0.099   -9.86e-07    1.15e-05\n",
       "HSSVAL                                            -5.631e-06   3.77e-06     -1.494      0.135    -1.3e-05    1.76e-06\n",
       "HWSVAL                                              9.47e-09   3.24e-07      0.029      0.977   -6.26e-07    6.45e-07\n",
       "=====================================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = sm.GLM.from_formula(formula=\"UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + UEDUC3 + GENDER\", data=medical, family=sm.families.Binomial())\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple splits we can use train_test_split. This mthod will split the data set of features and a column with our dependant variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24550, 29) (10522, 29)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8241923844201573"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "X_train, X_test, y_train, y_test = train_test_split(medical, medical.UCURNINS, test_size=0.3, random_state=0)\n",
    "print(X_train.shape, X_test.shape)\n",
    "mod = sm.GLM.from_formula(formula=\"UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + UEDUC3 + GENDER\", data=X_train, family=sm.families.Binomial())\n",
    "res = mod.fit()\n",
    "res.summary()\n",
    "preds = res.predict(X_test)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the random state or initialize it with a random number. having controll over randomness of our splits can help a lot with reproducibility in some cases. To see the differenc run a couple times a cell above and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24550, 29) (10522, 29)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8290065956805197"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "X_train, X_test, y_train, y_test = train_test_split(medical, medical.UCURNINS, test_size=0.3, random_state=random.randint(0,1000))\n",
    "print(X_train.shape, X_test.shape)\n",
    "mod = sm.GLM.from_formula(formula=\"UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + UEDUC3 + GENDER\", data=X_train, family=sm.families.Binomial())\n",
    "res = mod.fit()\n",
    "res.summary()\n",
    "preds = res.predict(X_test)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can very easily emply stratify with just one argument. We just neet to set the column for stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24550, 29) (10522, 29)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8260032858195256"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(medical, medical.UCURNINS, stratify=medical.UCURNINS, test_size=0.3, random_state=random.randint(0,10000))\n",
    "print(X_train.shape, X_test.shape)\n",
    "mod = sm.GLM.from_formula(formula=\"UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + UEDUC3 + GENDER\", data=X_train, family=sm.families.Binomial())\n",
    "res = mod.fit()\n",
    "res.summary()\n",
    "preds = res.predict(X_test)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we cen see, in case of logistic regression on this data set there is very little difference with and withour stratification. Our \"1\" label is represented well enough (5 000 observations) for randomness to work well enough with 0.7 and 0.3 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30006\n",
       "1     5066\n",
       "Name: UCURNINS, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical.UCURNINS.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the tendency to overfitting in logistic regression. As you can see below we need to reduct the train data set size to only 10% to really start overfitting. This show great resiliance to overfitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.8301312975102032 Valid AUC: 0.8195855991304546\n",
      "Train AUC: 0.8305351187585147 Valid AUC: 0.8228293433443608\n",
      "Train AUC: 0.8305996904150379 Valid AUC: 0.8241923844201573\n",
      "Train AUC: 0.8292820001302296 Valid AUC: 0.8273335897502359\n",
      "Train AUC: 0.8285465450375926 Valid AUC: 0.8277940615469459\n",
      "Train AUC: 0.8326912531816938 Valid AUC: 0.8245681334347579\n",
      "Train AUC: 0.8326216671491147 Valid AUC: 0.8250584921627727\n",
      "Train AUC: 0.8350450051645271 Valid AUC: 0.8226559394521357\n",
      "Train AUC: 0.8514010513685688 Valid AUC: 0.8205487796228925\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(medical, medical.UCURNINS, test_size=0.1*k, random_state=0)\n",
    "    mod = sm.GLM.from_formula(formula=\"UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + UEDUC3 + GENDER\", data=X_train, family=sm.families.Binomial())\n",
    "    res = mod.fit()\n",
    "    predsTrain = res.predict()\n",
    "    preds = res.predict(X_test)\n",
    "    print(\"Train AUC:\", roc_auc_score(y_train, predsTrain), \"Valid AUC:\", roc_auc_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For KFold CV we do not get datasets but rather indices for the sets. This is done to avoid unecessary duplication of datasets. Lets run 10-fold CV and see the results. For KFold we need to add shuffle=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.8289092423753209 Valid AUC: 0.8298359903707848\n",
      "Train AUC: 0.8285525616753869 Valid AUC: 0.8329893929030385\n",
      "Train AUC: 0.8284397138931388 Valid AUC: 0.8336957509552151\n",
      "Train AUC: 0.8292869173025139 Valid AUC: 0.8257056606954467\n",
      "Train AUC: 0.8287059085147939 Valid AUC: 0.8307897176779526\n",
      "Train AUC: 0.8288606257209882 Valid AUC: 0.8306593465167103\n",
      "Train AUC: 0.8288966146718248 Valid AUC: 0.8289385802485897\n",
      "Train AUC: 0.8301284005088319 Valid AUC: 0.8177175612298072\n",
      "Train AUC: 0.8294214810790544 Valid AUC: 0.8243445859780784\n",
      "Train AUC: 0.8297465431221138 Valid AUC: 0.8218262319487102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=random.randint(0,10000))\n",
    "\n",
    "for train, test in kf.split(medical.index.values):\n",
    "    mod = sm.GLM.from_formula(formula=\"UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + UEDUC3 + GENDER\", data=medical.iloc[train], family=sm.families.Binomial())\n",
    "    res = mod.fit()\n",
    "    predsTrain = res.predict()\n",
    "    preds = res.predict(medical.iloc[test])\n",
    "    print(\"Train AUC:\", roc_auc_score(medical.iloc[train].UCURNINS, predsTrain), \"Valid AUC:\", roc_auc_score(medical.iloc[test].UCURNINS, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can see the results vary a lot from sample to sample. W see values from 0.81 to 0.838 on validation sets.\n",
    "\n",
    "To understand the problem of variance even more lets see the results for 5-fold CV. The variance of results seems slightly lower for 5-fold CV. This is natural as logistic regression does not overfit too much and we increase the size ov validation set (that stabilizes the results on validation samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.8282991038904495 Valid AUC: 0.8317725532311208\n",
      "Train AUC: 0.8302651221544994 Valid AUC: 0.8230673002574298\n",
      "Train AUC: 0.8301739424175641 Valid AUC: 0.8237197710749945\n",
      "Train AUC: 0.8277196084328705 Valid AUC: 0.8335902366863904\n",
      "Train AUC: 0.829373126154837 Valid AUC: 0.8262556203315866\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=random.randint(0,10000))\n",
    "\n",
    "for train, test in kf.split(medical.index.values):\n",
    "    mod = sm.GLM.from_formula(formula=\"UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + UEDUC3 + GENDER\", data=medical.iloc[train], family=sm.families.Binomial())\n",
    "    res = mod.fit()\n",
    "    predsTrain = res.predict()\n",
    "    preds = res.predict(medical.iloc[test])\n",
    "    print(\"Train AUC:\", roc_auc_score(medical.iloc[train].UCURNINS, predsTrain), \"Valid AUC:\", roc_auc_score(medical.iloc[test].UCURNINS, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the problem of variance with more detail. Lets run 10-fold CV 10 times and se the variance of averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.8291109680876065 Valid AUC: 0.8272190041353833\n",
      "Train AUC: 0.8290980647872745 Valid AUC: 0.8274610474973002\n",
      "Train AUC: 0.829109293947934 Valid AUC: 0.8276598092411926\n",
      "Train AUC: 0.8291058359974182 Valid AUC: 0.8275475467898878\n",
      "Train AUC: 0.8290967151644495 Valid AUC: 0.8278207210367741\n",
      "Train AUC: 0.8291168249485061 Valid AUC: 0.8275534440576415\n",
      "Train AUC: 0.8291167123334713 Valid AUC: 0.8274208312865131\n",
      "Train AUC: 0.8290991801971505 Valid AUC: 0.8278776801203389\n",
      "Train AUC: 0.8291345888868127 Valid AUC: 0.8272198401409516\n",
      "Train AUC: 0.8291148498788798 Valid AUC: 0.8274019361772907\n"
     ]
    }
   ],
   "source": [
    "for z in range(10):\n",
    "    trainRes = []\n",
    "    valRes = []\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=random.randint(0,10000))\n",
    "    for train, test in kf.split(medical.index.values):\n",
    "        mod = sm.GLM.from_formula(formula=\"UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + UEDUC3 + GENDER\", data=medical.iloc[train], family=sm.families.Binomial())\n",
    "        res = mod.fit()\n",
    "        predsTrain = res.predict()\n",
    "        preds = res.predict(medical.iloc[test])\n",
    "        trainRes.append(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain))\n",
    "        valRes.append(roc_auc_score(medical.iloc[test].UCURNINS, preds))\n",
    "    print(\"Train AUC:\", np.mean(trainRes), \"Valid AUC:\", np.mean(valRes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do the same for 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.8291960162223658 Valid AUC: 0.827545249209928\n",
      "Train AUC: 0.8291705021980027 Valid AUC: 0.8274415890328379\n",
      "Train AUC: 0.8291669304656775 Valid AUC: 0.8278209910570794\n",
      "Train AUC: 0.8292226253823214 Valid AUC: 0.8272906261418982\n",
      "Train AUC: 0.8291382625334685 Valid AUC: 0.8278253630206279\n",
      "Train AUC: 0.8292194021408811 Valid AUC: 0.8270916143191627\n",
      "Train AUC: 0.8292113126334616 Valid AUC: 0.8273966307564091\n",
      "Train AUC: 0.8292079026872052 Valid AUC: 0.8275563752114172\n",
      "Train AUC: 0.8291791062147839 Valid AUC: 0.8275383378092833\n",
      "Train AUC: 0.8292354144261997 Valid AUC: 0.8273551062969563\n"
     ]
    }
   ],
   "source": [
    "for z in range(10):\n",
    "    trainRes = []\n",
    "    valRes = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random.randint(0,10000))\n",
    "    for train, test in kf.split(medical.index.values):\n",
    "        mod = sm.GLM.from_formula(formula=\"UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + UEDUC3 + GENDER\", data=medical.iloc[train], family=sm.families.Binomial())\n",
    "        res = mod.fit()\n",
    "        predsTrain = res.predict()\n",
    "        preds = res.predict(medical.iloc[test])\n",
    "        trainRes.append(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain))\n",
    "        valRes.append(roc_auc_score(medical.iloc[test].UCURNINS, preds))\n",
    "    print(\"Train AUC:\", np.mean(trainRes), \"Valid AUC:\", np.mean(valRes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that for logistc regression has very stable results for both 5 and 10 fold validations. Whats more it does nto overfit too much as the difference in the value of our metric for train and validation sets is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Exercises 4.\n",
    "\n",
    "# Exercise 4.1.\n",
    "\n",
    "# Titanic passengers data – 1310 observations and 15 variables:\n",
    "\n",
    "# passenger_id – Unique passenger id\n",
    "# pclass – Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "# survived – Survival (0 = No, 1 = Yes)\n",
    "# name – Name and SUrname\n",
    "# sex – Sex (0 = Male, 1 = Female)\n",
    "# age – Age in years\n",
    "# sibsp – # of siblings / spouses aboard the Titanic\n",
    "# parch – # of parents / children aboard the Titanic\n",
    "# ticket – Ticket number\n",
    "# fare – Passenger fare\n",
    "# cabin – Cabin number\n",
    "# embarked – Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "# boat – Lifeboat (if survived)\n",
    "# body – Body number (if did not survive and body was recovered)\n",
    "# home.dest – Home/Destination\n",
    "# \n",
    "# Re-run your best models for all algorithms for 5-fold CV. \n",
    "# Check the stability of results for repeated K-fold\n",
    "# Check in repeated k-fold CV if adding stratification changes your results (stability)\n",
    "# Check if you didnt overfit in your models. Check if you can imrpove you validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 4.2.\n",
    "# Wine Quality Data Set: \"data/wines.csv\"\n",
    "# source: https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "# The file contains data on samples of white and red Portuguese wine \n",
    "# Vinho Verde. \n",
    "# Various physico-chemical characteristics of individual samples\n",
    "# are available as well as wine quality scores on a point scale (0-10) \n",
    "# made by specialists.\n",
    "\n",
    "# Re-run your best models for all algorithms for 5-fold CV. \n",
    "# Check the stability of results for repeated K-fold\n",
    "# Check in repeated k-fold CV if adding stratification changes your results (stability).\n",
    "# Compare the effect of stratification with titanic problem.\n",
    "# Check if you didnt overfit in your models. Check if you can imrpove you validation score.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
