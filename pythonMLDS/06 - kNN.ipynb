{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wilam/.local/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-ticks')\n",
    "%matplotlib inline\n",
    "\n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes' 'No']\n",
      "[1 0]\n",
      "(35072, 29) 187.275198571514\n"
     ]
    }
   ],
   "source": [
    "medical = pd.read_pickle(\"data/medical.p\")\n",
    "print(medical.UCURNINS.unique())\n",
    "medical[\"UCURNINS\"] = (medical.UCURNINS==\"Yes\").astype(int)\n",
    "print(medical.UCURNINS.unique())\n",
    "features = [\"UMARSTAT\", \"USATMED\", \"URELATE\", \"REGION\", \"FHOSP\", \"FDENT\", \"FEMER\", \"FDOCT\", \"UIMMSTAT\", \"UAGE\", \"U_FTPT\", \"U_WKSLY\", \"U_USHRS\", \"HOTHVAL\", \"HRETVAL\", \"HSSVAL\", \"HWSVAL\", \"UBRACE\", \"UEDUC3\", \"GENDER\"]\n",
    "levCols = []\n",
    "numCols = []\n",
    "for col in features:\n",
    "    if medical[col].dtype==object:\n",
    "        levCols.append(col)\n",
    "    else:\n",
    "        numCols.append(col)\n",
    "\n",
    "# Binaryzacja zmiennych nominalnych\n",
    "dummLev = pd.get_dummies(medical[levCols], drop_first=True)\n",
    "dummLev.shape\n",
    "medical2 = pd.concat([medical[numCols], dummLev], axis=1)\n",
    "print(medical.shape, math.sqrt(medical.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we have our dataset prepared. With all nominal variables in binary form. \n",
    "\n",
    "We have around 35 k rows. That is a considerable amount so chosing $k=\\sqrt{N}$ might be too high. Maybe lets start with 50.\n",
    "\n",
    "First we need to standarize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medical2[numCols] = medical2[numCols].apply(lambda x: (x-x.mean())/x.std())\n",
    "features = medical2.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8100906626566241\n",
      "[0.8068274352412889, 0.8227779243154852, 0.8068194266434432, 0.7989174537503217, 0.8151110733325813]\n",
      "0.8636806785573544\n",
      "[0.8498930862437634, 0.8687099073414113, 0.8671228970630168, 0.8554319931565441, 0.8772455089820359]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "probs = []\n",
    "indicies = []\n",
    "aucs = []\n",
    "bacc = []\n",
    "accs = []\n",
    "n_neighbors = 50\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors, n_jobs=-1, p=2)\n",
    "for train, test in kf.split(medical2.index.values):\n",
    "    clf.fit(medical2.iloc[train][features].values, medical.iloc[train][\"UCURNINS\"].values)\n",
    "    prob = clf.predict_proba(medical2.iloc[test][features].values)\n",
    "    probs.append(prob)\n",
    "    indicies.append(test)\n",
    "    aucs.append(metrics.roc_auc_score(medical.iloc[test][\"UCURNINS\"].values, prob[:,1]))\n",
    "    accs.append(metrics.accuracy_score(medical.iloc[test][\"UCURNINS\"].values, (prob[:,1]>0.5).astype(int)))\n",
    "print(np.mean(aucs))\n",
    "print(aucs)\n",
    "print(np.mean(accs))\n",
    "print(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8036455110439705\n",
      "[0.8068274352412889, 0.8227779243154852, 0.8068194266434432, 0.7989174537503217, 0.8151110733325813, 0.7938441819174347, 0.8082769313070204, 0.7968999042853857, 0.7866600007247408, 0.8003207789220039]\n",
      "0.8642937015694707\n",
      "[0.8498930862437634, 0.8687099073414113, 0.8671228970630168, 0.8554319931565441, 0.8772455089820359, 0.8507483962936565, 0.8704205274411975, 0.8671228970630168, 0.8579982891360137, 0.8782435129740519]\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 25\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors, n_jobs=-1, p=2)\n",
    "for train, test in kf.split(medical2.index.values):\n",
    "    clf.fit(medical2.iloc[train][features].values, medical.iloc[train][\"UCURNINS\"].values)\n",
    "    prob = clf.predict_proba(medical2.iloc[test][features].values)\n",
    "    probs.append(prob)\n",
    "    indicies.append(test)\n",
    "    aucs.append(metrics.roc_auc_score(medical.iloc[test][\"UCURNINS\"].values, prob[:,1]))\n",
    "    accs.append(metrics.accuracy_score(medical.iloc[test][\"UCURNINS\"].values, (prob[:,1]>0.5).astype(int)))\n",
    "print(np.mean(aucs))\n",
    "print(aucs)\n",
    "print(np.mean(accs))\n",
    "print(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now 50 seems to be the best value.\n",
    "\n",
    "Now lets see if changing our distance metric from Euclidesian to Manhatan distance is any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8106434294762069\n",
      "[0.8068274352412889, 0.8227779243154852, 0.8068194266434432, 0.7989174537503217, 0.8151110733325813, 0.7938441819174347, 0.8082769313070204, 0.7968999042853857, 0.7866600007247408, 0.8003207789220039, 0.8200906612887722, 0.8342881707866624, 0.8208078470921378, 0.8137184018116637, 0.8342912507241628]\n",
      "0.8648401964018202\n",
      "[0.8498930862437634, 0.8687099073414113, 0.8671228970630168, 0.8554319931565441, 0.8772455089820359, 0.8507483962936565, 0.8704205274411975, 0.8671228970630168, 0.8579982891360137, 0.8782435129740519, 0.8528866714183891, 0.8702779757662152, 0.8714000570287995, 0.8570002851439977, 0.8781009409751924]\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 50\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors, n_jobs=-1, p=1)\n",
    "for train, test in kf.split(medical2.index.values):\n",
    "    clf.fit(medical2.iloc[train][features].values, medical.iloc[train][\"UCURNINS\"].values)\n",
    "    prob = clf.predict_proba(medical2.iloc[test][features].values)\n",
    "    probs.append(prob)\n",
    "    indicies.append(test)\n",
    "    aucs.append(metrics.roc_auc_score(medical.iloc[test][\"UCURNINS\"].values, prob[:,1]))\n",
    "    accs.append(metrics.accuracy_score(medical.iloc[test][\"UCURNINS\"].values, (prob[:,1]>0.5).astype(int)))\n",
    "print(np.mean(aucs))\n",
    "print(aucs)\n",
    "print(np.mean(accs))\n",
    "print(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a different metric did not change a lot. Lets see how much improvement can we get by chosing fewer variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8077414892545176\n",
      "[0.8068274352412889, 0.8227779243154852, 0.8068194266434432, 0.7989174537503217, 0.8151110733325813, 0.7938441819174347, 0.8082769313070204, 0.7968999042853857, 0.7866600007247408, 0.8003207789220039, 0.8200906612887722, 0.8342881707866624, 0.8208078470921378, 0.8137184018116637, 0.8342912507241628, 0.799687035079033, 0.8133730986451371, 0.7734201269074457, 0.7939119577993796, 0.8147861245162522]\n",
      "0.8655482335400473\n",
      "[0.8498930862437634, 0.8687099073414113, 0.8671228970630168, 0.8554319931565441, 0.8772455089820359, 0.8507483962936565, 0.8704205274411975, 0.8671228970630168, 0.8579982891360137, 0.8782435129740519, 0.8528866714183891, 0.8702779757662152, 0.8714000570287995, 0.8570002851439977, 0.8781009409751924, 0.8561653599429794, 0.8746970776906628, 0.8699743370402053, 0.8568577131451383, 0.8806672369546621]\n"
     ]
    }
   ],
   "source": [
    "feat2 = [\"USATMED\", \"FDENT\", \"UEDUC3\", \"FDOCT\", 'UMARSTAT_Married_live together','UIMMSTAT_Foreign-born, non-citizen',\n",
    " 'UIMMSTAT_US-born citizen']\n",
    "n_neighbors = 30\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors, n_jobs=-1, p=1)\n",
    "for train, test in kf.split(medical2.index.values):\n",
    "    clf.fit(medical2.iloc[train][feat2].values, medical.iloc[train][\"UCURNINS\"].values)\n",
    "    prob = clf.predict_proba(medical2.iloc[test][feat2].values)\n",
    "    probs.append(prob)\n",
    "    indicies.append(test)\n",
    "    aucs.append(metrics.roc_auc_score(medical.iloc[test][\"UCURNINS\"].values, prob[:,1]))\n",
    "    accs.append(metrics.accuracy_score(medical.iloc[test][\"UCURNINS\"].values, (prob[:,1]>0.5).astype(int)))\n",
    "print(np.mean(aucs))\n",
    "print(aucs)\n",
    "print(np.mean(accs))\n",
    "print(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the results is not much worse. Additionally for fairly large datasets our estimator is rather stable with different values of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises 6\n",
    "### Exercise 6.1\n",
    "\n",
    "Import titanic dataset, divide it into training and test data. Apply the KNN method to predict the value of the variable ‘survived’, test a few values of parameter k. Check whether the rescaling of variables allows to get better forecasts in the test sample.\n",
    "\n",
    "### Exercise 6.2\n",
    "\n",
    "Find the optimal value for the parameter k using cross validation.\n",
    "\n",
    "### Exercise 6.3\n",
    "\n",
    "Calculate the prediction error on the train and test sample for k = 1, 2, …. 100, and plot results. What conclusions can be drawn from this result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
