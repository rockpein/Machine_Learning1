{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost functions and evaluation metrics\n",
    "## How are the algorithms trained?\n",
    "We already established that our goal is to create an estimator $\\hat{F}(X)$, that tries to approximate a true and unobservable function $F(X)$. In supervised learning we use a training set for which true values are given as $y$. For the same set of data our estimator gives predictions $\\hat{y}$ and therefore we can calculate the prediction error $y-\\hat{y}$.\n",
    "\n",
    "There are many algorithms for finding $\\hat{F}(X)$. Every one of them uses a different approach or recipe to generate the estimator. However, no matter which algorithm we use and how we go about construction of $\\hat{F}(X)$ one thing remains constant. We want the prediction error to be as small as possible.\n",
    "\n",
    "With this goal in mind we chose a cost function also called a loss function. This framing is easy to remember as we want our error to be as low as possible and hence we want to minimize cost/error/loss.\n",
    "\n",
    "When we say we want our estimator to be as good as possible we need to optimize it. Optimization is nothing else but minimization of the cost function.\n",
    "\n",
    "No matter what type of algorithm we use, weather there is an analytical solution or only a numerical approximation we always optimize a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of the cost function\n",
    "Choosing a right function is crucial as the cost function **directly affects the our estimator $\\hat{F}(X)$**. Therefore, whenever possible, we will choose a function for which it is possible to prove that our estimator is unbiased - its parameters are equal to true values.\n",
    "\n",
    "$$E(\\hat\\beta)=\\beta $$\n",
    "\n",
    "In best case scenario we would also like for the loss function to create an *minimum-variance unbiased estimator (MVUE)*.\n",
    "\n",
    "Last but not least, as optimization algorithms relay on differentiation we want the cost function to be smooth. When our estimator doesn't have this property our (numerical) optimizer may not converge at all or stop in some local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic cost functions\n",
    "### Mean Squared Error (MSE)\n",
    "One of the most commonly used cost functions is *mean squared error (MSE)*. Well known metric used in most basic econometric method of linear regression *ordinary least squares (OLS)*.\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y_i})^2 $$\n",
    "The reason for this method popularity is tha fact that this estimator is unbiased and efficient.\n",
    "\n",
    "Those advantages do not meant that we can call MSE \"the best cost function\". One of its biggest shortcomings is the fact that it puts quite a lot of weight towards outliers.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "One solution for over-weighting of outliers is using mean absolute error.\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y_i}| $$\n",
    "However this is also not a perfect function, mainly due to the fact that it is not differentialble in zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of loss functions\n",
    "In practical applications of machine learning we should never forget why we are engaged in predictive modeling. What is the *real* or business meaning of prediction errors? What does it mean for use, business wise, that we made a prediction error. Do we really make twice as big of a mistake when the prediction error increases two fold? Maybe it is the opposite? Maybe the bigger the prediction error the less important it is?\n",
    "\n",
    "We can also easily imagine a situation in which the importance of our prediction error would be asymmetric. For instance in credit scoring predicting too low is not as bad as predicting too high. Similarly making a type I error can be better/worse than making type II error. In these situations we can create a custom asymmetrical cost function to incorporate this asymmetry.\n",
    "\n",
    "To sum up a chosen cost function should always **reflect the real cost of prediction errors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function vs evaluation metrics\n",
    "In theory it is possible to apply almost any cost function to most ML algorithms. However in most cases it would not be a successful implementation (mainly due to function being non-smooth). In practice unless there is a very strong business need for custom cost function, a basic function would be used. However we widen our knowledge about estimators using additional evaluation metrics.\n",
    "\n",
    "We calculate evaluation metrics after the estimator is already created with use of different cost function. The **evaluation metrics do not affect the form of estimator $\\hat{F}(X)$**.\n",
    "\n",
    "Evaluation functions are usually called metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics in regression\n",
    "In regression problems most common metrics are:\n",
    "* MSE - Mean Square Error\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y_i})^2 $$\n",
    "* MAE - Mean Absolute Error\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y_i}| $$\n",
    "* MedAE - Median Absolute Error - this metric is immune to over-weighting of outliers.\n",
    "$$MedAE = median( |y_0 - \\hat{y_0}| \\dots |y_n - \\hat{y_n}| )$$\n",
    "* MSLE - Mean Squared Logarithmic Error - and example of an asymmetric metric the bigger the error the, relatively, lower the wieght.\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n (\\log(1+y_i) - \\log(1+\\hat{y_i}))^2 $$\n",
    "* $R^2$ - Coefficient of determination\n",
    "$$R^2 \\equiv 1 - {RSS \\over TSS}$$\n",
    "where:\n",
    "$$\\bar{y}=\\frac{1}{n}\\sum_{i=1}^n y_i$$\n",
    "$$RSS=\\sum_i (y_i-\\hat{y_i})^2$$\n",
    "$$TSS=\\sum_i (y_i-\\bar{y})^2$$\n",
    "\n",
    "In case of regression problems the interpretation of the distribution of prediction error is straightforward. Most people intuitively understand what is mean square error. In any time we can plot a histogram of our prediction errors and we will be able to get a complete picture of the performance of regression estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics in classification\n",
    "On the other hand in classification problems it is not that easy to asses the prediction error, especially when by predictions we understand binary labels. In this case the first measure that comes to mind is accuracy. Lets see how our predictions can look like in binary classification.\n",
    "\n",
    "![CONFMATRIX](img/confMatrix2.png)\n",
    "Źródło: http://www.ritchieng.com/machine-learning-evaluate-classification-model/\n",
    "\n",
    "Symbols:\n",
    "$TP$ - true positive, $TN$ - True negatives, $FP$ - False positives, $FN$ - False negatives, $P$ - number of positives, $N$ - number of negatives\n",
    "\n",
    "* Acc - Accuracy\n",
    "$$ Acc = {(TP+TN) \\over (P+N)}$$\n",
    "It should be quite clear that accuracy is not a very good metric. For well balanced data sets we intuitive interpretation of accuracy is correct. however of imbalanced datasets accuracy can quickly become almost useless. Imagine a dataset in which there are ninety-nine \"1\" and one \"0\". We can get 99% accuracy by simply assigning \"1\" to every observation. Balanced accuracy tries to alleviate this problem.\n",
    "\n",
    "* Bacc - Balanced Accuracy\n",
    "$$ Bacc = ({TP \\over P} + {TN \\over N})/2$$\n",
    "\n",
    "When we look ar \"ones\" and \"zeros\" together we can get a good picture of the performance of our estimator. We can try to look what share of labels did we get right:\n",
    "* TPR - True Positive Rate - Sensitivity, Hit Rate, Recall\n",
    "$$ TPR = {TP \\over P} = {TP \\over TP+FN}$$\n",
    "* TNR (SPC) - True Negative Rate - Specificity\n",
    "$$TNR = {TN \\over N} = {TN \\over FP+TN}$$\n",
    "\n",
    "Another approach is to use total number of predictions in current group\n",
    "* PPV - Positive Predictive Value - Precision\n",
    "$$ PPV = {TP \\over TP + FP}$$\n",
    "* NPV - Negative Predictive Value \n",
    "$$ NPV = {TN \\over TN+FN}$$\n",
    "\n",
    "From time to time we can also see a following metric:\n",
    "* F1 - f-score\n",
    "$$F_1 = 2 * {(precision * recall) \\over (precision + recall)}$$\n",
    "\n",
    "A table below is a nice summary of classification metrics:\n",
    "![measureMatrix](img/measuresMatrix.png)\n",
    "Source: https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation in classification - cost functions and probabilities\n",
    "### Logloss, cross entropy\n",
    "When we look at our prediction in terms of labels we lose a very important information - what probability did the estimator assign to a given observation. All ML algorithms \"work\" using probabilities, rational numbers between 0 and 1, and not labels. Only at the end we chose a cut-off point and assign labels based on probabilities. For one of the \"1\" the probability can be equal to -.55 and it can be 0.87 for other. We can say that in the second case the algorithm was more certain. We lose this information while working with just labels.\n",
    "\n",
    "So lets take a look at the most important cost functions in classification. in logistic regression we were optimizing probabilities by maximizing the likelihood of observing the data in training set. In practice due to the shape of logistic function our cost function was given as:\n",
    "$$f^*_\\text{Logistic}= \\ln\\left(\\frac{p(1\\mid x)}{1-p(1\\mid x)}\\right) = \\ln\\left(p(1\\mid x)\\right)-\\ln\\left(1-p(1\\mid x)\\right)$$\n",
    "\n",
    "Now lets take a look at the equation for *cross entropy* in binary problem for which the cost function will look like this:\n",
    "\n",
    "$$J = -\\frac1n\\sum_{i=1}^n\\ \\bigg[y_i  \\log \\hat y_i + (1 - y_i)  \\log (1 - \\hat y_i)\\bigg]$$\n",
    "As you can see in binary classification either first or second term of the sum will be equal to zero. As our true values are either 0 or 1 we will have either $\\log \\hat y_i$ or $\\log (1 - \\hat y_i)$. Therefore the algorithm will try to get as close as possible to true probability. One can say that this function is very similar to MSLE. \n",
    "\n",
    "Whats more we can very simply generalize this function to a multinominal problem:\n",
    "\n",
    "$$J = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{k=1}^{K} y_{i,k} \\log \\hat y_{i,k}$$\n",
    "\n",
    "In this case we cannot do a p, 1-p \"trick\" so we will just look at the log of the probability of a true class. It will be zero for all other classes. This equation is usually called multinominal logloss. Main advantage is its smoothness.\n",
    "\n",
    "\n",
    "### Hinge loss\n",
    "In case of logloss the value of cost function changes continuously no matter weather we make good or bad predictions. Hinge loss behaves differently in that respect. We assume that our target values ar -1 or 1. Than for one observation we have\n",
    "$$l(y) = \\max(0, 1-y \\cdot \\hat y)$$\n",
    "As you can see in hinge loss our loss is zero when we assign high enough probability. Therefore the algorithm focuses only on the \"harder\" observations to separate. This loss function is used in a fairly important algorithm *Support Vecotr Machine (SVM)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for probabilities\n",
    "In case of classification we do not usually use logloss nor hing loss for classification. It is very hard to interpret their values directly and almost impossible to compare between datasets. Instead we use one of the following metrics.\n",
    "\n",
    "### ROC\n",
    "* ROC - Receiver Operating Characteristic Curve\n",
    "* AUC ROC - Area Under Curve - It is the area under curve ROC. Usually when we see just AUC it means AUC ROC. It takes values from 0.5 to1. In case where we want to describe a quality of binary classification with just one number it is one of the best metrics.\n",
    "![ROC](img/roc.png)\n",
    "Source: http://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics\n",
    "\n",
    "ROC tells us how the values of TPR and FPR (FPR = 1- TNR) change with the rise of the cut off point. As you can see the curve always go to the corners of the square. At the begning when the cutoff point is 1 we classify every observation as \"0\". Obviously in this situation TNR = 1 (FPR  = 0). With the decrease of the cutoff point we increase the number of \"1\" - the TPR starts to increase. However our estimator will probably not be perfect so some of predicted \"1\" are incorrect, therefore the increase of FPR (and decrease of TNR).\n",
    "\n",
    "A perfect estimator would go from lower-bottom corner to top-right through top-left corner.\n",
    "\n",
    "### Lift \n",
    "Lift is strictly connected to AUC. It focuses on relative performance to random estimator. usually we plot Lift curve for deciles. One value tells the ratio of correctly predicted ones by our estimator in comparison to random estimator. A value 2.5 for second decil means that wen we chose 20% of predictions with highest probabilities we predicted 50% of all \"1\" (TPR = 0.5)\n",
    "\n",
    "![LIFT](img/lift.png)\n",
    "Source: https://www.neuraldesigner.com/blog/methods-binary-classification \n",
    "The values of Lift we can read from ROC by looking vertically and comparing the value of ROC to the diagonal.\n",
    "\n",
    "### Precision/recall curve - AUC PR\n",
    "The last metric, that is recently gaining in popularity, is precision/recall curve. On one hand it is very similar to ROC as we use the same points (ordered probabilities). However instead of using TPR and TNR we look ad TPR (Recall) and PPV (Precision).\n",
    " \n",
    "<img src=\"img/precRecallCurve.png\" width=\"45%\">\n",
    "Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "\n",
    "In most cases there will be little difference in the quality of our model and ranking order of models between AUC ROC and AUC PR. however when the data set is heavily unbalanced we should chose our metrics very carefully. The research shows that we can get very peculiar behavior. For instance in figure below we see two algorithms working on dataset with 20 \"1\" and 2000 \"0\". The AUC ROC values are 0.813 for algorithm  I and 0.875 for II, and AUC PR are  0.51 and 0.03 respectively.\n",
    "\n",
    "<img src=\"img/ROCPRREC.png\">\n",
    "Source: Davis, J., & Goadrich, M. (2006, June). The relationship between Precision-Recall and ROC curves. In Proceedings of the 23rd international conference on Machine learning (pp. 233-240). ACM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Excercise\n",
    "Lets go back to our logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-ticks')\n",
    "%matplotlib inline\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medical = pd.read_pickle(\"data/medical.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The file includes socio-demographic data, including \n",
    "# health insurance and various aspects of health care\n",
    "# touchpoints for the respondent group of a survey\n",
    "# conducted in the USA.\n",
    "\n",
    "# The collection includes 35072 observations and 27 variables:\n",
    "  \n",
    "# UMARSTAT – Marital status recode\n",
    "# UCUREMP – Currently has employer coverage\n",
    "# UCURNINS – Currently uninsured\n",
    "# USATMED – Satisfied with quality of medical care\n",
    "# URELATE – Number of relatives in household\n",
    "# REGION – region\n",
    "# STATE - state\n",
    "# HHID – Household identification number\n",
    "# FHOSP – In hospital overnight last year\n",
    "# FDENT – Dental visits last year\n",
    "# FEMER – Number of emergency room visits last year\n",
    "# FDOCT – Number of doctor visits last year\n",
    "# UIMMSTAT – Immigration status\n",
    "# U_USBORN – U.S.- or foreign-born\n",
    "# UAGE – Age topcoded\n",
    "# U_FTPT – Full-time or part-time worker this year\n",
    "# U_WKSLY – Weeks worked last year\n",
    "# U_HRSLY – Hours worked per week last year\n",
    "# U_USHRS – Hours worked per week this year\n",
    "# HEARNVAL – Earnings amount last year - Household\n",
    "# HOTHVAL – Household income, total exc. earnings\n",
    "# HRETVAL – Retirement amount – Household\n",
    "# HSSVAL – Social Security amount - Household\n",
    "# HWSVAL – Wages and salaries amount – Household\n",
    "# UBRACE – race\n",
    "# GENDER – gender\n",
    "# UEDUC3 – education level\n",
    "# CEYES - color of eyes\n",
    "# CHAIR - color of hair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod = sm.OLS.from_formula(formula='U_USHRS ~ UAGE + U_WKSLY + U_HRSLY + HEARNVAL + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + FDENT + FEMER + FDOCT + UMARSTAT + UCUREMP + URELATE + REGION + FHOSP + UIMMSTAT + U_FTPT + UBRACE + GENDER + C(UEDUC3) + CEYES + CHAIR', data=medical)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod = sm.OLS.from_formula(formula='U_USHRS ~ UAGE + U_WKSLY + U_HRSLY + HEARNVAL + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + FDENT + FEMER + FDOCT + UMARSTAT + UCUREMP + URELATE + REGION + FHOSP + UIMMSTAT + U_FTPT + UBRACE + GENDER + C(UEDUC3) + CEYES + CHAIR', data=medical)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "preds = res.predict()\n",
    "print(\"R^2: %.2f\" % metrics.r2_score(medical[\"U_USHRS\"].values, preds))\n",
    "print(\"Mean squared error: %.2f\" % metrics.mean_squared_error(medical[\"U_USHRS\"].values, preds))\n",
    "print(\"Mean absolute error: %.2f\" % metrics.mean_absolute_error(medical[\"U_USHRS\"].values, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medical[\"U_USHRS\"].hist(bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(preds, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Mean squared log error: %.2f\" % metrics.mean_squared_log_error(medical[\"U_USHRS\"].values, preds))\n",
    "except:\n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod = sm.GLM.from_formula(formula=\"UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + UEDUC3 + GENDER\", data=medical, family=sm.families.Binomial())\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = res.predict()\n",
    "# Statsmodels took No as 1 due to alphabetic sorting. Hence >\n",
    "preds = np.array([1 if x<0.5 else 0 for x in probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab((medical.UCURNINS==\"Yes\").astype(int), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, _ = roc_curve((medical.UCURNINS==\"No\").astype(int), probs)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score((medical.UCURNINS==\"No\").astype(int), probs)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve((medical.UCURNINS==\"No\").astype(int), probs)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.8,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.25,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Exercises 4.\n",
    "\n",
    "# Exercise 4.1.\n",
    "\n",
    "# Titanic passengers data – 1310 observations and 15 variables:\n",
    "\n",
    "# passenger_id – Unique passenger id\n",
    "# pclass – Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "# survived – Survival (0 = No, 1 = Yes)\n",
    "# name – Name and SUrname\n",
    "# sex – Sex (0 = Male, 1 = Female)\n",
    "# age – Age in years\n",
    "# sibsp – # of siblings / spouses aboard the Titanic\n",
    "# parch – # of parents / children aboard the Titanic\n",
    "# ticket – Ticket number\n",
    "# fare – Passenger fare\n",
    "# cabin – Cabin number\n",
    "# embarked – Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "# boat – Lifeboat (if survived)\n",
    "# body – Body number (if did not survive and body was recovered)\n",
    "# home.dest – Home/Destination\n",
    "\n",
    "# For you best models from multinominal logistic regression and LDE\n",
    "# Prepare Confusion matrix\n",
    "# Plot a ROC Curve (with AUC)\n",
    "# Plot Precision Recall Curve (with AUC)\n",
    "# Calculate log loss, accuracy and balanced accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 4.2.\n",
    "# Wine Quality Data Set: \"data/wines.csv\"\n",
    "# source: https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "# The file contains data on samples of white and red Portuguese wine \n",
    "# Vinho Verde. \n",
    "# Various physico-chemical characteristics of individual samples\n",
    "# are available as well as wine quality scores on a point scale (0-10) \n",
    "# made by specialists.\n",
    "\n",
    "# For you best models from multinominal logistic regression and LDE\n",
    "# calcualte TPR and PPV for every class\n",
    "# calcualte multinominal logloss for your best models.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
